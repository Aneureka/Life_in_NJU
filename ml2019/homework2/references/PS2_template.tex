\documentclass[a4paper,UTF8]{article}
\usepackage{ctex}
\usepackage[margin=1.25in]{geometry}
\usepackage{color}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{enumerate}
\usepackage{bm}
\usepackage{hyperref}
\usepackage{epsfig}
\usepackage{color}
\usepackage{mdframed}
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{bm}
\newmdtheoremenv{thm-box}{Theorem}
\newmdtheoremenv{prop-box}{Proposition}
\newmdtheoremenv{def-box}{定义}
\DeclareMathOperator*{\argmax}{arg\,max}

\usepackage{listings}
\usepackage{xcolor}
\lstset{
	numbers=left,
	numberstyle= \tiny,
	keywordstyle= \color{ blue!70},
	commentstyle= \color{red!50!green!50!blue!50},
	frame=shadowbox, % 阴影效果
	rulesepcolor= \color{ red!20!green!20!blue!20} ,
	escapeinside=``, % 英文分号中可写入中文
	xleftmargin=2em,xrightmargin=2em, aboveskip=1em,
	framexleftmargin=2em
}

\usepackage{booktabs}

\setlength{\evensidemargin}{.25in}
\setlength{\textwidth}{6in}
\setlength{\topmargin}{-0.5in}
\setlength{\topmargin}{-0.5in}
% \setlength{\textheight}{9.5in}
%%%%%%%%%%%%%%%%%%此处用于设置页眉页脚%%%%%%%%%%%%%%%%%%
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{layout}
\footskip = 10pt
\pagestyle{fancy}                    % 设置页眉
\lhead{2018年春季}
\chead{机器学习导论}
% \rhead{第\thepage/\pageref{LastPage}页}
\rhead{作业二}
\cfoot{\thepage}
\renewcommand{\headrulewidth}{1pt}  			%页眉线宽，设为0可以去页眉线
\setlength{\skip\footins}{0.5cm}    			%脚注与正文的距离
\renewcommand{\footrulewidth}{0pt}  			%页脚线宽，设为0可以去页脚线

\makeatletter 									%设置双线页眉
\def\headrule{{\if@fancyplain\let\headrulewidth\plainheadrulewidth\fi%
\hrule\@height 1.0pt \@width\headwidth\vskip1pt	%上面线为1pt粗
\hrule\@height 0.5pt\@width\headwidth  			%下面0.5pt粗
\vskip-2\headrulewidth\vskip-1pt}      			%两条线的距离1pt
 \vspace{6mm}}     								%双线与下面正文之间的垂直间距
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\numberwithin{equation}{section}
%\usepackage[thmmarks, amsmath, thref]{ntheorem}
\newtheorem{theorem}{Theorem}
\newtheorem*{definition}{Definition}
\newtheorem*{solution}{Solution}
\newtheorem*{prove}{Proof}
\newcommand{\indep}{\rotatebox[origin=c]{90}{$\models$}}

\usepackage{multirow}

%--

%--
\begin{document}
\title{机器学习导论\\
作业二}
\author{学号, 作者姓名, 邮箱}
\maketitle

\section{[25pts] Multi-Class Logistic Regression}
教材的章节3.3介绍了对数几率回归解决二分类问题的具体做法。假定现在的任务不再是二分类问题，而是多分类问题，其中标记$y\in\{1,2\dots,K\}$。请将对数几率回归算法拓展到该多分类问题。

\begin{enumerate}[(1)]
	\item \textbf{[15pts]} 给出该对率回归模型的“对数似然”(log-likelihood);
	\item \textbf{[10pts]} 计算出该“对数似然”的梯度。
\end{enumerate}

提示1：假设该多分类问题满足如下$K-1$个对数几率，
\begin{eqnarray*}
	\ln\frac{p(y=1|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_1^\mathrm{T}\mathbf{x}+b_1\\
	\ln\frac{p(y=2|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_2^\mathrm{T}\mathbf{x}+b_2\\
	&\dots&\\
	\ln\frac{p(y={K-1}|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\mathbf{w}_{K-1}^\mathrm{T}\mathbf{x}+b_{K-1}
\end{eqnarray*}

提示2：定义指示函数$\mathbb{I}(\cdot)$，
$$\mathbb{I}(y=j)=
\begin{cases}
1& \text{若$y$等于$j$}\\
0& \text{若$y$不等于$j$}
\end{cases}$$

\begin{solution}
	此处用于写解答(中英文均可)
~\\\item(1)令$\bm{\beta_i} = (\bm{w_i};b_i)$，$\hat{\mathbf{x}}=(\mathbf{x};1)$，则$\bm{w_i^T}\mathbf{x}+b_i$可简写为$\bm{\beta_i^T}\hat{\mathbf{x}}$。假设该多分类问题满足如下$K-1$个对数几率，
\begin{eqnarray*}
	\ln\frac{p(y=1|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\bm{\beta_1^T}\hat{\mathbf{x}}\\
	\ln\frac{p(y=2|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\bm{\beta_2^T}\hat{\mathbf{x}}\\
	&\dots&\\
	\ln\frac{p(y={K-1}|\mathbf{x})}{p(y=K|\mathbf{x})}&=&\bm{\beta_{K-1}^T}\hat{\mathbf{x}}
\end{eqnarray*}
则有
\[
    p(y=i|\mathbf{x}) = p(y=K|\mathbf{x})e^{\bm{\beta_i^T}\hat{\mathbf{x}}}
\]
得到其它K-1类的概率，可以推出
\[
    p(y=K|\bm{x}) = 1 - \sum_{i=1}^{K-1}p(y=K|\bm{x})e^{\bm{\beta_i^T}\hat{\bm{x}}} \Longrightarrow p(y=K|\bm{x}) = \frac{1}{1+\sum_{i=1}^{K-1}e^{\bm{\beta_i^T}\hat{\mathbf{x}}}}
\]
所以有
\begin{equation}
    p(y=i|\bm{x}) = \frac{e^{\bm{\beta_i^T}\hat{\mathbf{x}}}}{1+\sum_{i=1}^{K-1}e^{\bm{\beta_i^T}\hat{\mathbf{x}}}}
\end{equation}
令$\bm{\beta} = (\bm{\beta_1^T};\bm{\beta_2^T};\dots;\bm{\beta_{k-1}^T})$，则该对率回归模型的“对数似然”为
\begin{equation}
    \ell(\bm{\beta}) = \sum_{i=1}^{m}\ln p(y_i | \bm{x_i};\bm{\beta})
\end{equation}
定义指示函数$\mathbb{I}(\cdot)$，
$$\mathbb{I}(y=j)=
\begin{cases}
1& \text{若$y$等于$j$}\\
0& \text{若$y$不等于$j$}
\end{cases}$$
则式(1.2)中的似然项可重写为
\begin{equation}
    p(y_i | \bm{x_i};\bm{\beta}) = p(y=1|\bm{x_i})^{\mathbb{I}(y=1)}\cdot p(y=2|\bm{x_i})^{\mathbb{I}(y=2)}\cdots p(y=K|\bm{x_i})^{\mathbb{I}(y=K)}
\end{equation}
将式(1.3)代入(1.2)，并根据式(1.1)可得到最终结果
\begin{equation}
    \ell(\bm{\beta}) = \sum_{i=1}^{m}\left(\mathbb{I}(y=1)\bm{\beta_1^T}\hat{\bm{x_i}} + \mathbb{I}(y=2)\bm{\beta_2^T}\hat{\bm{x_i}} + \dots +\mathbb{I}(y=K-1)\bm{\beta_{K-1}^T}\hat{\bm{x_i}}-\ln (1+\sum_{j=1}^{K-1}e^{\bm{\beta_j^T}\hat{\bm{x_i}}})\right)
\end{equation}
(参考资料见\cite{ref1})
\\ \item(2)该“对数似然”的梯度为
\[
    \frac{\partial{\ell(\bm{\beta})}}{\partial{\bm{\beta_j}}} = \sum_{i=1}^{m}\hat{\bm{x_i}}\left(\mathbb{I}(y_i=j)-\frac{e^{\bm{\beta_j^T}\hat{\bm{x_i}}}}{1+\sum_{j=1}^{K-1}e^{\bm{\beta_j^T}\hat{\bm{x_i}}}}\right)
\]
其中$j=1, 2, \dots, K-1$.
\end{solution}
\newpage


\section{[20pts] Linear Discriminant Analysis}
假设有两类数据，正例独立同分布地从高斯分布$\mathcal{N}(\mu_1,\Sigma_1)$采样得到，负例独立同分布地从另一高斯分布$\mathcal{N}(\mu_2,\Sigma_2)$采样得到，其中参数$\mu_1,\Sigma_1$及$\mu_2,\Sigma_2$均已知。现在，我们定义“最优分类”：若对空间中的任意样本点，分别计算已知该样本采样于正例时该样本出现的概率与已知该样本采样于负例时该样本出现的概率后，取概率较大的所采类别作为最终预测的类别输出，则我们说这样的分类方式满足“最优分类”性质。

试证明：当两类数据的分布参数$\Sigma_1=\Sigma_2=\Sigma$时，线性判别分析 (LDA)方法满足“最优分类”性质。（提示：找到满足最优分类性质的分类平面。）
\begin{solution}
	此处用于写解答(中英文均可)
\\ \\
根据贝叶斯判定准则，对每个样本，需要选择使$P(c)P(\bm{x}|c)$取得最大值来得到最优分类，即
\begin{equation}
    h^*(\bm{x}) = \argmax_{c \in \mathcal{Y}} P(c)P(\bm{x}|c)
\end{equation}
由于两类数据都服从高斯分布，设$c \in \{1,2\}$表示类别，则$P(\bm{x}|c)$服从高斯分布，
\begin{equation}
    P(\bm{x}|c=1) = \frac{1}{(2\pi)^{d/2}|\Sigma_1|^{1/2}}e^{-\frac{1}{2}(\bm{x}-\mu_1)^T\Sigma_1^{-1}(\bm{x}-\mu_1)}
\end{equation}
\begin{equation}
P(\bm{x}|c=2) = \frac{1}{(2\pi)^{d/2}|\Sigma_2|^{1/2}}e^{-\frac{1}{2}(\bm{x}-\mu_2)^T\Sigma_1^{-1}(\bm{x}-\mu_2)}
\end{equation}
其中d是样例$\bm{x}$的维数.
将(2.2)代入(2.1)可得，
\begin{eqnarray*}
    h_1^*(\bm{x}) &=& \argmax P(c=1)P(\bm{x}|c=1)\\
    &=& \argmax \log(P(c=1)P(\bm{x}|c=1))   \\
    &=& \argmax \left(-\log ((2\pi)^{d/2}|\Sigma_1|^{1/2})-\frac{1}{2}(\bm{x}-\mu_1)^T\Sigma_1^{-1}(\bm{x}-\mu_1) + \log P(c=1)\right)\\
    &=& \argmax \left(-\frac{1}{2}(\bm{x}-\mu_1)^T\Sigma_1^{-1}(\bm{x}-\mu_1) + \log P(c=1)\right)\\
    &=& \argmax \left(\bm{x^T}\Sigma_1^{-1}\mu_1 - \frac{1}{2}\mu_1^T\Sigma_1^{-1}\mu_1+\log P(c=1)\right)
\end{eqnarray*}
同理有
\[
    h_2^*(\bm{x}) = \argmax \left(\bm{x^T}\Sigma_2^{-1}\mu_2 - \frac{1}{2}\mu_2^T\Sigma_2^{-1}\mu_2+\log P(c=2)\right)
\]
记下式为线性判别函数，$c \in \{1,2\}$
\[
    \delta_c(\bm{x}) = \bm{x^T}\Sigma_c^{-1}\mu_c - \frac{1}{2}\mu_c^T\Sigma_c^{-1}\mu_c+\log P(c)
\]
则类别1和类别2的决策边界为，
\[
    {\bm{x}: \delta_{c=1}(\bm{x}) = \delta_{c=2}(\bm{x})}
\]
当$\Sigma_1=\Sigma_2=\Sigma$时，有
\[
    \log \frac{P(c=1)}{P(c=2)} - \frac{1}{2}(\mu_1+\mu_2)^T\Sigma^{-1}(\mu_1-\mu_2)+\bm{x^T}\Sigma^{-1}(\mu_1-\mu_2)=0
\]
由贝叶斯判定准则可知，该决策边界对应最优分类平面，其法向量为
\[
    \bm{w^*} = \Sigma^{-1}(\mu_1-\mu_2)
\]
而$\bm{w^*}$是LDA关于这两类数据的一个解，所以当两类数据的分布参数$\Sigma_1=\Sigma_2=\Sigma$时，线性判别分析 (LDA)方法满足“最优分类”性质。
(参考资料见\cite{ref2})
\end{solution}
\newpage



\section{[55+10*pts] Logistic Regression Programming}
在本题中，我们将初步接触机器学习编程，首先我们需要初步了解机器学习编程的主要步骤，然后结合对数几率回归，在UCI数据集上进行实战。机器学习编程的主要步骤可参见\href{http://blog.csdn.net/cqy_chen/article/details/78690975}{博客}。

本次实验选取UCI数据集Page Blocks（\href{http://lamda.nju.edu.cn/ml2018/PS2/PS2_dataset.zip}{下载链接}）。数据集基本信息如表~\ref{data_inf}所示，此数据集特征维度为10维，共有5类样本，并且类别间样本数量不平衡。

\begin{table}[!h]
	\centering
	\caption{Page Blocks数据集中每个类别的样本数量。}\vspace{3mm}
	\label{data_inf}
	\begin{tabular}{l|cccccc}\hline
		标记     & 1    & 2   & 3  & 4  & 5   & total \\ \hline
		训练集   & 4431 & 292 & 25 & 84 & 103 & 4935  \\
		测试集   & 482  & 37  & 3  & 4  & 12  & 538   \\ \hline
	\end{tabular}
\end{table}

对数几率回归（Logistic Regression, LR）是一种常用的分类算法。面对多分类问题，结合处理多分类问题技术，利用常规的LR算法便能解决这类问题。

\begin{enumerate}[(1)]
    \item \textbf{[5pts]} 此次编程作业要求使用Python 3或者MATLAB编写，请将main函数所在文件命名为~LR\_main.py或者LR\_main.m，效果为运行此文件便能完成整个训练过程，并输出测试结果，方便作业批改时直接调用；	
	\item \textbf{[30pts]} 本题要求编程实现如下实验功能：
	\begin{itemize}
		\item \textbf{[10pts]} 根据《机器学习》3.3节，实现LR算法，优化算法可选择梯度下降，亦可选择牛顿法；
		\item \textbf{[10pts]} 根据《机器学习》3.5节，利用“一对其余”（One vs. Rest, OvR）策略对分类LR算法进行改进，处理此多分类任务；
		\item \textbf{[10pts]} 根据《机器学习》3.6节，在训练之前，请使用“过采样”（oversampling）策略进行样本类别平衡；
	\end{itemize}
	
	

	\item \textbf{[20pts]} 实验报告中报告算法的实现过程（能够清晰地体现 (1) 中实验要求，请勿张贴源码），如优化算法选择、相关超参数设置等，并填写表~\ref{exp_performance}，在\url{http://www.tablesgenerator.com/}上能够方便地制作LaTex表格；
	
	\item \textbf{[附加题 10pts]} 尝试其他类别不平衡问题处理策略（尝试方法可以来自《机器学习》也可来自其他参考材料），尽可能提高对少数样本的分类准确率，并在实验报告中给出实验设置、比较结果及参考文献；
\end{enumerate}
\noindent \textbf{[**注意**]} 本次实验除了numpy等数值处理工具包外禁止调用任何开源机器学习工具包，一经发现此实验题分数为0，请将实验所需所有源码文件与作业pdf文件放在同一个目录下，请勿将数据集放在提交目录中。
\newpage
\noindent{\textbf{实验报告.}}\\
(1)MATLAB实现LR算法，由OvR策略训练出五个分类器，即每次将一个类的样例作为正例、所有其他类的样例作为反例来训练五个分类器。根据“极大似然法”估计$\bm{\beta_i}(i=1,2,3,4,5)$，采用梯度下降法求得最优解，即
\[
    \bm{\beta_i} = \bm{\beta_i} - \alpha\frac{\partial\ell(\bm{\beta_i})}{\partial\bm{\beta_i}}.
\]
其中, $\beta$初始化为全1向量，步长$\alpha$设置为0.0001，终止距离$\epsilon$设置为0.001，当$\alpha\frac{\partial\ell(\bm{\beta_i})}{\partial\bm{\beta_i}}$ 向量的每个值都小于$\epsilon$，则终止算法，当前$\beta_i$向量即为最终结果。(“梯度下降”参考见\cite{ref3})\\
(2)过采样算法采用SMOTE算法，SMOTE算法中的k-近邻设置为$k=5$，然后对样例较少的3、4、5类进行过采样，N根据测试结果进行调整。
(“SMOTE算法”参考见\cite{ref4})\\
(3)根据求出的$\bm{\beta_i}(i=1,2,3,4,5)$结合LR算法估算测试样本的预测值，取预测值最大的类别作为预测类别，得到所有测试样本的预测类别后，再跟测试样本的真实类别进行比较，计算出查全率、查准率和准确率。
\begin{table}[!h]
	\centering
	\caption{算法在测试数据集上泛化性能测试结果，先报告在每个类别上的查全率和查准率，最后报告在整个测试数据集上的准确率。}\vspace{3mm}
	\label{exp_performance}
	\begin{tabular}{@{}c|cccccc@{}}
		\toprule
		标记     & 1    & 2    & 3    & 4    & 5    & \begin{tabular}[c]{@{}c@{}}准确率\end{tabular} \\ \midrule
		查全率    & 0.977178 & 0.810811 & 0.666667 & 1.00 & 0.416667 & \multirow{2}{*}{0.951673}                                     \\
		查准率 & 0.971134 & 0.937500 & 0.666667 & 0.40 & 0.625000 &  \\ \cmidrule(r){1-7}
	\end{tabular}
\end{table}

\begin{thebibliography}{99}
\bibitem{ref1}Multinomial logistic regression. \url{https://en.wikipedia.org/wiki/Multinomial_logistic_regression}.
\bibitem{ref2}CodeTutor. 线性判别分析（二）. CSDN博客, \url{https://blog.csdn.net/victoriaw/article/details/78275394}.
\bibitem{ref3}刘建平. 梯度下降小结. 博客园, \url{https://www.cnblogs.com/pinard/p/5970503.html}.
\bibitem{ref4}Determined22. 机器学习——类不平衡问题与SMOTE过采样算法. 博客园, \url{https://www.cnblogs.com/Determined22/p/5772538.html}.
\end{thebibliography}
\end{document}
